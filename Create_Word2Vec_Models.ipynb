{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8c2e0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_functions import create_corpus_file, process_dataset, reddit_data_cleaning, train_word2vec, load_thesis_datasets,\\\n",
    "generate_word2vector, generate_word_embeddings, setup_rnn, generate_rnn_model, dataset_shuffler\n",
    "\n",
    "import tqdm\n",
    "import logging\n",
    "import time\n",
    "import string\n",
    "import sys\n",
    "import re\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.word2vec import LineSentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e068b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the word2vec models we have trained\n",
    "\n",
    "wikicorpus = 'glove.6B.200d.txt'\n",
    "\n",
    "reddit_cbow_dim100_win5 = 'reddit_model_CBOW_dim100_con5.txt'\n",
    "reddit_cbow_dim100_win10 = 'reddit_model_CBOW_dim200_con10.txt'\n",
    "reddit_cbow_dim200_win5 = 'reddit_CBOW_DIM200_win5.txt'\n",
    "reddit_cbow_dim200_win10 = 'reddit_model_CBOW_dim200_con10.txt'\n",
    "reddit_bow_dim300_win10 = 'reddit_model_CBOW_dim300_con10.txt'\n",
    "reddit_sg_dim200_win3 = 'reddit_model_SG_dim200_win3.txt'\n",
    "reddit_sg_dim200_win5 = 'reddit_model_SG_dim200_win5.txt'\n",
    "reddit_sg_dim200_win10 = 'reddit_sg_dim300_win5.txt'\n",
    "reddit_sg_dim300_win5 = 'reddit_sg_dim300_win5.txt'\n",
    "reddit_sg_dim300_win10 = 'reddit_model_SG_dim300_win10.txt'\n",
    "\n",
    "transcript_sg_dim100_win5 = 'transcript_SG_DIM100_win5.txt'\n",
    "transcript_sg_dim100_win10 = 'transcript_SG_DIM100_win10.txt'\n",
    "transcript_sg_dim200_win5 = 'transcript_SG_DIM200_win5.txt'\n",
    "transcript_sg_dim200_win10 = 'transcript_SG_DIM200_win10.txt'\n",
    "transcript_sg_dim300_win5 = 'transcript_SG_DIM300_WIN5.txt'\n",
    "transcript_sg_dim300_win10 = 'transcript_sg_DIM300_win10-new.txt'\n",
    "transcript_cbow_dim100_win5 = 'transcript_CBOW_DIM100_win5.txt'\n",
    "transcript_cbow_dim100_win10 = 'transcript_CBOW_DIM100_win10.txt'\n",
    "transcript_cbow_dim200_win5 = 'transcript_CBOW_DIM200_win5.txt'\n",
    "transcript_cbow_dim200_win10 = 'transcript_CBOW_DIM200_win10.txt'\n",
    "transcript_cbow_dim300_win5 = 'transcript_CBOW_DIM300_win5.txt'\n",
    "transcript_cbow_dim300_win10 = 'transcript_CBOW_DIM300_win10.txt'\n",
    "\n",
    "\n",
    "# Load models and retrieve word vectors\n",
    "\n",
    "# REDDIT SKIP-GRAM MODELS\n",
    "\n",
    "# reddit_sg_dim100_win10_model = KeyedVectors.load(reddit_sg_dim100_win10)\n",
    "# reddit_sg_dim100_win10_vectors = reddit_sg_dim100_win10_model.wv\n",
    "\n",
    "reddit_sg_dim200_win5_model = KeyedVectors.load(reddit_sg_dim200_win5)\n",
    "reddit_sg_dim200_win5_vectors = reddit_sg_dim200_win5_model.wv\n",
    "\n",
    "reddit_sg_dim200_win10_model = KeyedVectors.load(reddit_sg_dim200_win10)\n",
    "reddit_sg_dim200_win10_vectors = reddit_sg_dim200_win10_model.wv\n",
    "\n",
    "reddit_sg_dim300_win5_model = KeyedVectors.load(reddit_sg_dim300_win5)\n",
    "reddit_sg_dim300_win5_vectors = reddit_sg_dim300_win5_model.wv\n",
    "\n",
    "reddit_sg_dim300_win10_model = KeyedVectors.load(reddit_sg_dim300_win10) #NOT WORKING\n",
    "reddit_sg_dim300_win10_vectors = reddit_sg_dim300_win10_model.wv\n",
    "\n",
    "reddit_sg_dim100_win10_model = KeyedVectors.load(reddit_sg_dim300_win5)\n",
    "reddit_sg_dim100_win10_vectors = reddit_sg_dim300_win5_model.wv\n",
    "\n",
    "# REDDIT CBOW MODELS\n",
    "\n",
    "# reddit_cbow_dim100_win5_model = KeyedVectors.load(reddit_cbow_dim100_win5)\n",
    "# reddit_cbow_dim100_win5_vectors = reddit_cbow_dim100_win5_model.wv\n",
    "\n",
    "# reddit_cbow_dim100_win10_model = KeyedVectors.load(reddit_cbow_dim100_win10)\n",
    "# reddit_cbow_dim100_win10_vectors = reddit_cbow_dim100_win10_model.wv\n",
    "\n",
    "# reddit_cbow_dim200_win5_model = KeyedVectors.load(reddit_cbow_dim200_win5)\n",
    "# reddit_cbow_dim200_win5_vectors = reddit_cbow_dim200_win5_model.wv\n",
    "\n",
    "# reddit_cbow_dim200_win10_model = KeyedVectors.load(reddit_cbow_dim200_win10)\n",
    "# reddit_cbow_dim200_win10_vectors = reddit_cbow_dim200_win10_model.wv\n",
    "\n",
    "reddit_bow_dim300_win10_model = KeyedVectors.load(reddit_bow_dim300_win10)\n",
    "reddit_bow_dim300_win10_vectors = reddit_bow_dim300_win10_model.wv\n",
    "\n",
    "# TRANSCRIPT SKIP-GRAM MODELS\n",
    "\n",
    "transcript_sg_dim100_win5_model = KeyedVectors.load(transcript_sg_dim100_win5)\n",
    "transcript_sg_dim100_win5_vectors = transcript_sg_dim100_win5_model.wv\n",
    "\n",
    "transcript_sg_dim100_win10_model = KeyedVectors.load(transcript_sg_dim100_win10)\n",
    "transcript_sg_dim100_win10_vectors = transcript_sg_dim100_win10_model.wv\n",
    "\n",
    "# transcript_sg_dim200_win5_model = KeyedVectors.load(transcript_sg_dim200_win5)\n",
    "# transcript_sg_dim200_win5_vectors = transcript_sg_dim200_win5_model.wv\n",
    "\n",
    "# transcript_sg_dim200_win10_model = KeyedVectors.load(transcript_sg_dim200_win10)\n",
    "# transcript_sg_dim200_win10_vectors = transcript_sg_dim200_win10_model.wv\n",
    "\n",
    "transcript_sg_dim300_win5_model = KeyedVectors.load(transcript_sg_dim300_win5)\n",
    "transcript_sg_dim300_win5_vectors = transcript_sg_dim300_win5_model.wv\n",
    "\n",
    "transcript_sg_dim300_win10_model = KeyedVectors.load(transcript_sg_dim300_win10)\n",
    "transcript_sg_dim300_win10_vectors = transcript_sg_dim300_win10_model.wv\n",
    "\n",
    "# TRANSCRIPT CBOW MODELS\n",
    "\n",
    "transcript_cbow_dim100_win5_model = KeyedVectors.load(transcript_cbow_dim100_win5)\n",
    "transcript_cbow_dim100_win5_vectors = transcript_cbow_dim100_win5_model.wv\n",
    "\n",
    "transcript_cbow_dim100_win10_model = KeyedVectors.load(transcript_cbow_dim100_win10)\n",
    "transcript_cbow_dim100_win10_vectors = transcript_cbow_dim100_win10_model.wv\n",
    "\n",
    "transcript_cbow_dim200_win5_model = KeyedVectors.load(transcript_cbow_dim200_win5)\n",
    "transcript_cbow_dim200_win5_vectors = transcript_cbow_dim200_win5_model.wv\n",
    "\n",
    "transcript_cbow_dim200_win10_model = KeyedVectors.load(transcript_cbow_dim200_win10)\n",
    "transcript_cbow_dim200_win10_vectors = transcript_cbow_dim200_win10_model.wv\n",
    "\n",
    "transcript_cbow_dim300_win5_model = KeyedVectors.load(transcript_cbow_dim300_win5)\n",
    "transcript_cbow_dim300_win5_vectors = transcript_cbow_dim300_win5_model.wv\n",
    "\n",
    "# transcript_cbow_dim300_win10_model = KeyedVectors.load(transcript_cbow_dim300_win10)\n",
    "# transcript_cbow_dim300_win10_vectors = transcript_cbow_dim300_win10_model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e498011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create corpus text file out of the processed reddit dataset (csv input, txt output)\n",
    "file_path = 'redditdf_forw2v.csv'\n",
    "create_corpus_file(file_path, 'reddit_corpus_source_final.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d5fbd8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/opc/my_functions.py:73: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data[cleaned_col] = data[cleaned_col].str.replace(pat, '')\n",
      "/home/opc/my_functions.py:74: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data[cleaned_col] = data[cleaned_col].str.replace(r'\\s+', ' ')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing completed in: 8 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Load out Thesis datasets\n",
    "df = process_dataset(load_thesis_datasets(), '2', '2-clean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76e0efd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-15 14:11:38,928 : INFO : collecting all words and their counts\n",
      "2021-06-15 14:11:38,930 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2021-06-15 14:11:39,126 : INFO : collected 43906 word types from a corpus of 1061606 raw words and 578 sentences\n",
      "2021-06-15 14:11:39,127 : INFO : Creating a fresh vocabulary\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-15 14:11:39,223 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=3 retains 20349 unique words (46.34674076436023%% of original 43906, drops 23557)', 'datetime': '2021-06-15T14:11:39.222486', 'gensim': '4.0.1', 'python': '3.7.10 (default, Feb 26 2021, 18:47:35) \\n[GCC 7.3.0]', 'platform': 'Linux-5.4.17-2102.200.13.el7uek.x86_64-x86_64-with-redhat-7.9-Maipo', 'event': 'prepare_vocab'}\n",
      "2021-06-15 14:11:39,223 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=3 leaves 1032054 word corpus (97.21629305034071%% of original 1061606, drops 29552)', 'datetime': '2021-06-15T14:11:39.223702', 'gensim': '4.0.1', 'python': '3.7.10 (default, Feb 26 2021, 18:47:35) \\n[GCC 7.3.0]', 'platform': 'Linux-5.4.17-2102.200.13.el7uek.x86_64-x86_64-with-redhat-7.9-Maipo', 'event': 'prepare_vocab'}\n",
      "2021-06-15 14:11:39,340 : INFO : deleting the raw counts dictionary of 43906 items\n",
      "2021-06-15 14:11:39,341 : INFO : sample=0.001 downsamples 41 most-common words\n",
      "2021-06-15 14:11:39,341 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 951846.8223694287 word corpus (92.2%% of prior 1032054)', 'datetime': '2021-06-15T14:11:39.341701', 'gensim': '4.0.1', 'python': '3.7.10 (default, Feb 26 2021, 18:47:35) \\n[GCC 7.3.0]', 'platform': 'Linux-5.4.17-2102.200.13.el7uek.x86_64-x86_64-with-redhat-7.9-Maipo', 'event': 'prepare_vocab'}\n",
      "2021-06-15 14:11:39,530 : INFO : estimated required memory for 20349 words and 300 dimensions: 59012100 bytes\n",
      "2021-06-15 14:11:39,531 : INFO : resetting layer weights\n",
      "2021-06-15 14:11:39,566 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2021-06-15T14:11:39.566271', 'gensim': '4.0.1', 'python': '3.7.10 (default, Feb 26 2021, 18:47:35) \\n[GCC 7.3.0]', 'platform': 'Linux-5.4.17-2102.200.13.el7uek.x86_64-x86_64-with-redhat-7.9-Maipo', 'event': 'build_vocab'}\n",
      "2021-06-15 14:11:39,566 : INFO : Word2Vec lifecycle event {'msg': 'training model with 8 workers on 20349 vocabulary and 300 features, using sg=1 hs=0 sample=0.001 negative=5 window=10', 'datetime': '2021-06-15T14:11:39.566935', 'gensim': '4.0.1', 'python': '3.7.10 (default, Feb 26 2021, 18:47:35) \\n[GCC 7.3.0]', 'platform': 'Linux-5.4.17-2102.200.13.el7uek.x86_64-x86_64-with-redhat-7.9-Maipo', 'event': 'train'}\n",
      "2021-06-15 14:11:40,674 : INFO : EPOCH 1 - PROGRESS: at 12.28% examples, 72976 words/s, in_qsize 15, out_qsize 0\n",
      "2021-06-15 14:11:41,715 : INFO : EPOCH 1 - PROGRESS: at 27.51% examples, 103062 words/s, in_qsize 15, out_qsize 0\n",
      "2021-06-15 14:11:42,762 : INFO : EPOCH 1 - PROGRESS: at 32.01% examples, 104051 words/s, in_qsize 15, out_qsize 0\n",
      "2021-06-15 14:11:43,829 : INFO : EPOCH 1 - PROGRESS: at 37.20% examples, 108294 words/s, in_qsize 15, out_qsize 0\n",
      "2021-06-15 14:11:44,942 : INFO : EPOCH 1 - PROGRESS: at 53.81% examples, 108911 words/s, in_qsize 15, out_qsize 0\n",
      "2021-06-15 14:11:45,969 : INFO : EPOCH 1 - PROGRESS: at 77.68% examples, 112721 words/s, in_qsize 15, out_qsize 0\n",
      "2021-06-15 14:11:47,091 : INFO : EPOCH 1 - PROGRESS: at 86.51% examples, 109941 words/s, in_qsize 10, out_qsize 0\n",
      "2021-06-15 14:11:47,140 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2021-06-15 14:11:47,165 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2021-06-15 14:11:47,188 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2021-06-15 14:11:47,341 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2021-06-15 14:11:47,392 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-06-15 14:11:47,405 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-06-15 14:11:47,436 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-06-15 14:11:47,502 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-06-15 14:11:47,503 : INFO : EPOCH - 1 : training on 1061606 raw words (905012 effective words) took 7.9s, 114095 effective words/s\n",
      "2021-06-15 14:11:48,619 : INFO : EPOCH 2 - PROGRESS: at 12.28% examples, 72387 words/s, in_qsize 15, out_qsize 0\n",
      "2021-06-15 14:11:49,642 : INFO : EPOCH 2 - PROGRESS: at 27.51% examples, 103466 words/s, in_qsize 15, out_qsize 0\n",
      "2021-06-15 14:11:50,685 : INFO : EPOCH 2 - PROGRESS: at 32.35% examples, 103074 words/s, in_qsize 15, out_qsize 0\n",
      "2021-06-15 14:11:51,814 : INFO : EPOCH 2 - PROGRESS: at 37.20% examples, 107023 words/s, in_qsize 15, out_qsize 0\n",
      "2021-06-15 14:11:52,833 : INFO : EPOCH 2 - PROGRESS: at 53.81% examples, 109774 words/s, in_qsize 15, out_qsize 0\n",
      "2021-06-15 14:11:53,844 : INFO : EPOCH 2 - PROGRESS: at 72.32% examples, 109432 words/s, in_qsize 15, out_qsize 0\n",
      "2021-06-15 14:11:54,858 : INFO : EPOCH 2 - PROGRESS: at 83.22% examples, 110222 words/s, in_qsize 12, out_qsize 0\n",
      "2021-06-15 14:11:55,121 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2021-06-15 14:11:55,138 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2021-06-15 14:11:55,139 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2021-06-15 14:11:55,305 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2021-06-15 14:11:55,312 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-06-15 14:11:55,347 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-06-15 14:11:55,383 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-06-15 14:11:55,430 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-06-15 14:11:55,430 : INFO : EPOCH - 2 : training on 1061606 raw words (904613 effective words) took 7.9s, 114158 effective words/s\n",
      "2021-06-15 14:11:56,554 : INFO : EPOCH 3 - PROGRESS: at 12.28% examples, 71974 words/s, in_qsize 15, out_qsize 0\n",
      "2021-06-15 14:11:57,565 : INFO : EPOCH 3 - PROGRESS: at 26.99% examples, 100522 words/s, in_qsize 15, out_qsize 0\n",
      "2021-06-15 14:11:58,570 : INFO : EPOCH 3 - PROGRESS: at 32.01% examples, 106411 words/s, in_qsize 15, out_qsize 0\n",
      "2021-06-15 14:11:59,628 : INFO : EPOCH 3 - PROGRESS: at 37.02% examples, 107622 words/s, in_qsize 15, out_qsize 0\n",
      "2021-06-15 14:12:00,636 : INFO : EPOCH 3 - PROGRESS: at 52.42% examples, 110751 words/s, in_qsize 15, out_qsize 0\n",
      "2021-06-15 14:12:01,675 : INFO : EPOCH 3 - PROGRESS: at 70.42% examples, 108909 words/s, in_qsize 15, out_qsize 0\n",
      "2021-06-15 14:12:02,800 : INFO : EPOCH 3 - PROGRESS: at 83.22% examples, 110076 words/s, in_qsize 12, out_qsize 0\n",
      "2021-06-15 14:12:03,083 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2021-06-15 14:12:03,133 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2021-06-15 14:12:03,164 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2021-06-15 14:12:03,232 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2021-06-15 14:12:03,257 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-06-15 14:12:03,262 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-06-15 14:12:03,330 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-06-15 14:12:03,351 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-06-15 14:12:03,352 : INFO : EPOCH - 3 : training on 1061606 raw words (905122 effective words) took 7.9s, 114319 effective words/s\n",
      "2021-06-15 14:12:04,486 : INFO : EPOCH 4 - PROGRESS: at 12.28% examples, 71332 words/s, in_qsize 15, out_qsize 0\n",
      "2021-06-15 14:12:05,640 : INFO : EPOCH 4 - PROGRESS: at 27.68% examples, 99922 words/s, in_qsize 15, out_qsize 0\n",
      "2021-06-15 14:12:06,711 : INFO : EPOCH 4 - PROGRESS: at 33.74% examples, 107923 words/s, in_qsize 15, out_qsize 0\n",
      "2021-06-15 14:12:07,750 : INFO : EPOCH 4 - PROGRESS: at 37.72% examples, 106902 words/s, in_qsize 15, out_qsize 0\n",
      "2021-06-15 14:12:08,789 : INFO : EPOCH 4 - PROGRESS: at 56.40% examples, 110581 words/s, in_qsize 15, out_qsize 0\n",
      "2021-06-15 14:12:09,941 : INFO : EPOCH 4 - PROGRESS: at 78.55% examples, 111713 words/s, in_qsize 15, out_qsize 0\n",
      "2021-06-15 14:12:10,946 : INFO : EPOCH 4 - PROGRESS: at 88.75% examples, 111364 words/s, in_qsize 8, out_qsize 0\n",
      "2021-06-15 14:12:10,955 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2021-06-15 14:12:10,978 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2021-06-15 14:12:11,000 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2021-06-15 14:12:11,091 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2021-06-15 14:12:11,135 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-06-15 14:12:11,169 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-06-15 14:12:11,188 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-06-15 14:12:11,231 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-06-15 14:12:11,232 : INFO : EPOCH - 4 : training on 1061606 raw words (904934 effective words) took 7.9s, 114889 effective words/s\n",
      "2021-06-15 14:12:12,376 : INFO : EPOCH 5 - PROGRESS: at 12.80% examples, 70713 words/s, in_qsize 15, out_qsize 0\n",
      "2021-06-15 14:12:13,525 : INFO : EPOCH 5 - PROGRESS: at 27.68% examples, 98218 words/s, in_qsize 15, out_qsize 0\n",
      "2021-06-15 14:12:14,634 : INFO : EPOCH 5 - PROGRESS: at 33.74% examples, 106558 words/s, in_qsize 15, out_qsize 0\n",
      "2021-06-15 14:12:15,723 : INFO : EPOCH 5 - PROGRESS: at 39.27% examples, 105350 words/s, in_qsize 15, out_qsize 0\n",
      "2021-06-15 14:12:16,776 : INFO : EPOCH 5 - PROGRESS: at 56.40% examples, 108330 words/s, in_qsize 15, out_qsize 0\n",
      "2021-06-15 14:12:17,914 : INFO : EPOCH 5 - PROGRESS: at 78.55% examples, 110164 words/s, in_qsize 15, out_qsize 0\n",
      "2021-06-15 14:12:18,785 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2021-06-15 14:12:18,810 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2021-06-15 14:12:18,941 : INFO : EPOCH 5 - PROGRESS: at 94.81% examples, 112410 words/s, in_qsize 5, out_qsize 1\n",
      "2021-06-15 14:12:18,942 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2021-06-15 14:12:18,951 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2021-06-15 14:12:19,001 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-06-15 14:12:19,028 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-06-15 14:12:19,080 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-06-15 14:12:19,081 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-06-15 14:12:19,081 : INFO : EPOCH - 5 : training on 1061606 raw words (905052 effective words) took 7.8s, 115357 effective words/s\n",
      "2021-06-15 14:12:19,082 : INFO : Word2Vec lifecycle event {'msg': 'training on 5308030 raw words (4524733 effective words) took 39.5s, 114507 effective words/s', 'datetime': '2021-06-15T14:12:19.082218', 'gensim': '4.0.1', 'python': '3.7.10 (default, Feb 26 2021, 18:47:35) \\n[GCC 7.3.0]', 'platform': 'Linux-5.4.17-2102.200.13.el7uek.x86_64-x86_64-with-redhat-7.9-Maipo', 'event': 'train'}\n",
      "2021-06-15 14:12:19,082 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec(vocab=20349, vector_size=300, alpha=0.025)', 'datetime': '2021-06-15T14:12:19.082690', 'gensim': '4.0.1', 'python': '3.7.10 (default, Feb 26 2021, 18:47:35) \\n[GCC 7.3.0]', 'platform': 'Linux-5.4.17-2102.200.13.el7uek.x86_64-x86_64-with-redhat-7.9-Maipo', 'event': 'created'}\n",
      "/home/opc/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `init_sims` (Gensim 4.0.0 implemented internal optimizations that make calls to init_sims() unnecessary. init_sims() is now obsoleted and will be completely removed in future versions. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
      "2021-06-15 14:12:19,093 : WARNING : destructive init_sims(replace=True) deprecated & no longer required for space-efficiency\n",
      "2021-06-15 14:12:19,098 : INFO : Word2Vec lifecycle event {'fname_or_handle': 'testest.txt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2021-06-15T14:12:19.098134', 'gensim': '4.0.1', 'python': '3.7.10 (default, Feb 26 2021, 18:47:35) \\n[GCC 7.3.0]', 'platform': 'Linux-5.4.17-2102.200.13.el7uek.x86_64-x86_64-with-redhat-7.9-Maipo', 'event': 'saving'}\n",
      "2021-06-15 14:12:19,098 : INFO : not storing attribute cum_table\n",
      "2021-06-15 14:12:19,127 : INFO : saved testest.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 0.6700055599212646 minutes\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train our Word2Vec Conspiracy Corpus\n",
    "\n",
    "# reddit_corpus = LineSentence('reddit_corpus_source_final.txt', max_sentence_length=21000) #Uncomment if you want to use the Reddit Corpus\n",
    "\n",
    "import multiprocessing\n",
    "start = time.time()\n",
    "\n",
    "#Set the logging format to get some basic updates.\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 300    # Dimensionality of the hidden layer representation\n",
    "min_word_count = 3   # Minimum word count to keep a word in the vocabulary\n",
    "num_workers = multiprocessing.cpu_count()       # Number of threads to run in parallel set to total number of cpus.\n",
    "context = 10          # Context window size (on each side)                                                       \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "sg = 1 # 1 for skip-gram, 0 for CBOW\n",
    "compute_loss = True # computes and saves the last loss function\n",
    "epochs = 3\n",
    "# Initialize and train the model. \n",
    "#The LineSentence object allows us to pass in a file name directly as input to Word2Vec,\n",
    "#instead of having to read it into memory first.\n",
    "\n",
    "print(\"Training model...\");\n",
    "model = word2vec.Word2Vec((other_corpus), workers=num_workers, vector_size=num_features, min_count = min_word_count, window = context, sample = downsampling, compute_loss = compute_loss, sg = sg)\n",
    "\n",
    "# We don't plan on training the model any further, so calling \n",
    "# init_sims will make the model more memory efficient by normalizing the vectors in-place.\n",
    "model.init_sims(replace=True);\n",
    "\n",
    "# Save the model\n",
    "model_name = \"testest.txt\"\n",
    "model.save(model_name)\n",
    "\n",
    "print('Total time: ' + str(((time.time() - start)/60)) + ' minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4bbd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the file created\n",
    "model_file = 'transcript_sg_DIM300_win10-new.txt'\n",
    "model = gensim.models.Word2Vec.load(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa89bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check for word-vectors\n",
    "model_vectors = model.wv\n",
    "model_vectors.most_similar('conspiracy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "more-cowbell",
   "language": "python",
   "name": "more-cowbell"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
