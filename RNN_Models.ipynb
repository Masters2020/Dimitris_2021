{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ae3b2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_functions import create_corpus_file, process_dataset, reddit_data_cleaning, train_word2vec, load_thesis_datasets,\\\n",
    "generate_word2vector, generate_word_embeddings, setup_rnn, generate_rnn_model, dataset_shuffler\n",
    "\n",
    "import time\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "\n",
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Dense, Dropout, Reshape, Flatten, concatenate, Input, Embedding, LeakyReLU\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14278e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we Load the word2vec models we have trained\n",
    "\n",
    "wikicorpus = 'glove.6B.200d.txt'\n",
    "\n",
    "reddit_cbow_dim100_win5 = 'reddit_model_CBOW_dim100_con5.txt'\n",
    "reddit_cbow_dim100_win10 = 'reddit_model_CBOW_dim200_con10.txt'\n",
    "reddit_cbow_dim200_win5 = 'reddit_CBOW_DIM200_win5.txt'\n",
    "reddit_cbow_dim200_win10 = 'reddit_model_CBOW_dim200_con10.txt'\n",
    "reddit_bow_dim300_win10 = 'reddit_model_CBOW_dim300_con10.txt'\n",
    "reddit_sg_dim200_win3 = 'reddit_model_SG_dim200_win3.txt'\n",
    "reddit_sg_dim200_win5 = 'reddit_model_SG_dim200_win5.txt'\n",
    "reddit_sg_dim200_win10 = 'reddit_sg_dim300_win5.txt'\n",
    "reddit_sg_dim300_win5 = 'reddit_sg_dim300_win5.txt'\n",
    "reddit_sg_dim300_win10 = 'reddit_model_SG_dim300_win10.txt'\n",
    "\n",
    "transcript_sg_dim100_win5 = 'transcript_SG_DIM100_win5.txt'\n",
    "transcript_sg_dim100_win10 = 'transcript_SG_DIM100_win10.txt'\n",
    "transcript_sg_dim200_win5 = 'transcript_SG_DIM200_win5.txt'\n",
    "transcript_sg_dim200_win10 = 'transcript_SG_DIM200_win10.txt'\n",
    "transcript_sg_dim300_win5 = 'transcript_SG_DIM300_WIN5.txt'\n",
    "transcript_sg_dim300_win10 = 'transcript_sg_DIM300_win10-new.txt'\n",
    "transcript_cbow_dim100_win5 = 'transcript_CBOW_DIM100_win5.txt'\n",
    "transcript_cbow_dim100_win10 = 'transcript_CBOW_DIM100_win10.txt'\n",
    "transcript_cbow_dim200_win5 = 'transcript_CBOW_DIM200_win5.txt'\n",
    "transcript_cbow_dim200_win10 = 'transcript_CBOW_DIM200_win10.txt'\n",
    "transcript_cbow_dim300_win5 = 'transcript_CBOW_DIM300_win5.txt'\n",
    "transcript_cbow_dim300_win10 = 'transcript_CBOW_DIM300_win10.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35166f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models and retrieve word vectors\n",
    "# Un-comment the word-vectors you wish to use\n",
    "\n",
    "# REDDIT SKIP-GRAM MODELS\n",
    "\n",
    "# reddit_sg_dim100_win10_model = KeyedVectors.load(reddit_sg_dim100_win10)\n",
    "# reddit_sg_dim100_win10_vectors = reddit_sg_dim100_win10_model.wv\n",
    "\n",
    "# reddit_sg_dim200_win5_model = KeyedVectors.load(reddit_sg_dim200_win5)\n",
    "# reddit_sg_dim200_win5_vectors = reddit_sg_dim200_win5_model.wv\n",
    "\n",
    "# reddit_sg_dim200_win10_model = KeyedVectors.load(reddit_sg_dim200_win10)\n",
    "# reddit_sg_dim200_win10_vectors = reddit_sg_dim200_win10_model.wv\n",
    "\n",
    "# reddit_sg_dim300_win5_model = KeyedVectors.load(reddit_sg_dim300_win5)\n",
    "# reddit_sg_dim300_win5_vectors = reddit_sg_dim300_win5_model.wv\n",
    "\n",
    "# reddit_sg_dim300_win10_model = KeyedVectors.load(reddit_sg_dim300_win10) #NOT WORKING\n",
    "# reddit_sg_dim300_win10_vectors = reddit_sg_dim300_win10_model.wv\n",
    "\n",
    "# reddit_sg_dim100_win10_model = KeyedVectors.load(reddit_sg_dim300_win5)\n",
    "# reddit_sg_dim100_win10_vectors = reddit_sg_dim300_win10_model.wv\n",
    "\n",
    "# REDDIT CBOW MODELS\n",
    "\n",
    "# reddit_cbow_dim100_win5_model = KeyedVectors.load(reddit_cbow_dim100_win5)\n",
    "# reddit_cbow_dim100_win5_vectors = reddit_cbow_dim100_win5_model.wv\n",
    "\n",
    "# reddit_cbow_dim100_win10_model = KeyedVectors.load(reddit_cbow_dim100_win10)\n",
    "# reddit_cbow_dim100_win10_vectors = reddit_cbow_dim100_win10_model.wv\n",
    "\n",
    "# reddit_cbow_dim200_win5_model = KeyedVectors.load(reddit_cbow_dim200_win5)\n",
    "# reddit_cbow_dim200_win5_vectors = reddit_cbow_dim200_win5_model.wv\n",
    "\n",
    "# reddit_cbow_dim200_win10_model = KeyedVectors.load(reddit_cbow_dim200_win10)\n",
    "# reddit_cbow_dim200_win10_vectors = reddit_cbow_dim200_win10_model.wv\n",
    "\n",
    "# reddit_bow_dim300_win10_model = KeyedVectors.load(reddit_bow_dim300_win10)\n",
    "# reddit_bow_dim300_win10_vectors = reddit_bow_dim300_win10_model.wv\n",
    "\n",
    "# TRANSCRIPT SKIP-GRAM MODELS\n",
    "\n",
    "# transcript_sg_dim100_win5_model = KeyedVectors.load(transcript_sg_dim100_win5)\n",
    "# transcript_sg_dim100_win5_vectors = transcript_sg_dim100_win5_model.wv\n",
    "\n",
    "# transcript_sg_dim100_win10_model = KeyedVectors.load(transcript_sg_dim100_win10)\n",
    "# transcript_sg_dim100_win10_vectors = transcript_sg_dim100_win10_model.wv\n",
    "\n",
    "# transcript_sg_dim200_win5_model = KeyedVectors.load(transcript_sg_dim200_win5)\n",
    "# transcript_sg_dim200_win5_vectors = transcript_sg_dim200_win5_model.wv\n",
    "\n",
    "# transcript_sg_dim200_win10_model = KeyedVectors.load(transcript_sg_dim200_win10)\n",
    "# transcript_sg_dim200_win10_vectors = transcript_sg_dim200_win10_model.wv\n",
    "\n",
    "# transcript_sg_dim300_win5_model = KeyedVectors.load(transcript_sg_dim300_win5)\n",
    "# transcript_sg_dim300_win5_vectors = transcript_sg_dim300_win5_model.wv\n",
    "\n",
    "# transcript_sg_dim300_win10_model = KeyedVectors.load(transcript_sg_dim300_win10)\n",
    "# transcript_sg_dim300_win10_vectors = transcript_sg_dim300_win10_model.wv\n",
    "\n",
    "# TRANSCRIPT CBOW MODELS\n",
    "\n",
    "# transcript_cbow_dim100_win5_model = KeyedVectors.load(transcript_cbow_dim100_win5)\n",
    "# transcript_cbow_dim100_win5_vectors = transcript_cbow_dim100_win5_model.wv\n",
    "\n",
    "# transcript_cbow_dim100_win10_model = KeyedVectors.load(transcript_cbow_dim100_win10)\n",
    "# transcript_cbow_dim100_win10_vectors = transcript_cbow_dim100_win10_model.wv\n",
    "\n",
    "# transcript_cbow_dim200_win5_model = KeyedVectors.load(transcript_cbow_dim200_win5)\n",
    "# transcript_cbow_dim200_win5_vectors = transcript_cbow_dim200_win5_model.wv\n",
    "\n",
    "# transcript_cbow_dim200_win10_model = KeyedVectors.load(transcript_cbow_dim200_win10)\n",
    "# transcript_cbow_dim200_win10_vectors = transcript_cbow_dim200_win10_model.wv\n",
    "\n",
    "# transcript_cbow_dim300_win5_model = KeyedVectors.load(transcript_cbow_dim300_win5)\n",
    "# transcript_cbow_dim300_win5_vectors = transcript_cbow_dim300_win5_model.wv\n",
    "\n",
    "transcript_cbow_dim300_win10_model = KeyedVectors.load(transcript_cbow_dim300_win10)\n",
    "transcript_cbow_dim300_win10_vectors = transcript_cbow_dim300_win10_model.wv\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d352cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load WikiCorpus vectors\n",
    "tmp_file = get_tmpfile(\"test_word2vec.txt\")\n",
    "_ = glove2word2vec(wikicorpus, tmp_file)\n",
    "wikicorpus_model = KeyedVectors.load_word2vec_format(tmp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c82c1bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/opc/my_functions.py:73: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data[cleaned_col] = data[cleaned_col].str.replace(pat, '')\n",
      "/home/opc/my_functions.py:74: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data[cleaned_col] = data[cleaned_col].str.replace(r'\\s+', ' ')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing completed in: 6 seconds.\n"
     ]
    }
   ],
   "source": [
    "df = process_dataset(load_thesis_datasets(), '2', '2-cleaned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39ce22f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 21000\n",
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "463a5bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting our dataset\n",
    "\n",
    "data_train, data_test = train_test_split(df, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f22cfb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "877230 words total, with a vocabulary size of 40012\n",
      "Max sentence length is 20148\n",
      "184376 words total, with a vocabulary size of 18219\n",
      "Max sentence length is 16679\n",
      "Found 40012 unique tokens.\n",
      "(40013, 300)\n"
     ]
    }
   ],
   "source": [
    "# Build our training vocabulary\n",
    "training_words = [word for tokens in data_train['tokens'] for word in tokens]\n",
    "training_sentence_lengths = [len(tokens) for tokens in data_train['tokens']]\n",
    "TRAINING_VOCAB = sorted(list(set(training_words)))\n",
    "print('%s words total, with a vocabulary size of %s' % (len(training_words), len(TRAINING_VOCAB)))\n",
    "print('Max sentence length is %s' % max(training_sentence_lengths))\n",
    "\n",
    "\n",
    "# Build our test vocabulary\n",
    "test_words = [word for tokens in data_test['tokens'] for word in tokens]\n",
    "test_sentence_lengths = [len(tokens) for tokens in data_test['tokens']]\n",
    "TEST_VOCAB = sorted(list(set(test_words)))\n",
    "print('%s words total, with a vocabulary size of %s' % (len(test_words), len(TEST_VOCAB)))\n",
    "print('Max sentence length is %s' % max(test_sentence_lengths))\n",
    "\n",
    "# PASS THE VECTORS FROM WORD2VEC HERE TO GENERATE TRAINING EMBEDDINGS\n",
    "training_embeddings = generate_word_embeddings(transcript_cbow_dim300_win10_vectors, data_train, generate_missing=True)\n",
    "tokenizer = Tokenizer(num_words=len(TRAINING_VOCAB), lower=True, char_level=True)\n",
    "tokenizer.fit_on_texts(data_train[\"tokens\"].tolist())\n",
    "training_sequences = tokenizer.texts_to_sequences(data_train[\"tokens\"].tolist())\n",
    "train_word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(train_word_index))\n",
    "\n",
    "train_rnn_data = pad_sequences(training_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "train_embedding_weights = np.zeros((len(train_word_index)+1, EMBEDDING_DIM))\n",
    "\n",
    "for word,index in train_word_index.items():\n",
    "    train_embedding_weights[index,:] = transcript_cbow_dim300_win10_vectors[word] if word in transcript_cbow_dim300_win10_vectors\\\n",
    "    else np.random.rand(EMBEDDING_DIM)\n",
    "print(train_embedding_weights.shape)\n",
    "\n",
    "test_sequences = tokenizer.texts_to_sequences(data_test[\"2-cleaned\"].tolist())\n",
    "test_rnn_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "label_names = ['Consp', 'NonConsp']\n",
    "y_train = data_train[label_names].values\n",
    "y_test = data_train[label_names].values\n",
    "x_train = train_rnn_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3837100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 21000)]           0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 21000, 300)        12003900  \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 12,105,918\n",
      "Trainable params: 102,018\n",
      "Non-trainable params: 12,003,900\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Initiate our RNN Classifier\n",
    "model = setup_rnn(train_embedding_weights, MAX_SEQUENCE_LENGTH, len(train_word_index)+1, EMBEDDING_DIM, \n",
    "                len(list(label_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5bcbc60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify epochs and batch-size\n",
    "num_epochs = 10\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2928cad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 2/12 [====>.........................] - ETA: 5:25 - loss: 0.6968 - acc: 0.4922 "
     ]
    }
   ],
   "source": [
    "# Train the RNN\n",
    "hist = model.fit(x_train, y_train, epochs=num_epochs, validation_split=0.2, shuffle=True, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80781903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to save model weights after training\n",
    "model_json = model.to_json()\n",
    "with open(\"modelname_weights.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bf3fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"modelname_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c662c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CODE FOR SAVING THE RESULTS OF OUR RNN\n",
    "# # serialize model to JSON\n",
    "# model_json = model.to_json()\n",
    "# with open(\"model.json\", \"w\") as json_file:\n",
    "#     json_file.write(model_json)\n",
    "# # serialize weights to HDF5\n",
    "# model.save_weights(\"model.h5\")\n",
    "# print(\"Saved model to disk\")\n",
    " \n",
    "# # later...\n",
    " \n",
    "# # load json and create model\n",
    "# json_file = open('model.json', 'r')\n",
    "# loaded_model_json = json_file.read()\n",
    "# json_file.close()\n",
    "# loaded_model = model_from_json(loaded_model_json)\n",
    "# # load weights into new model\n",
    "# loaded_model.load_weights(\"model.h5\")\n",
    "# print(\"Loaded model from disk\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "more-cowbell",
   "language": "python",
   "name": "more-cowbell"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
