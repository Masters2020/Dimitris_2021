{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8953404e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_functions import create_corpus_file, process_dataset, reddit_data_cleaning, train_word2vec, load_thesis_datasets,\\\n",
    "generate_word2vector, generate_word_embeddings, setup_rnn, generate_rnn_model, dataset_shuffler\n",
    "\n",
    "import logging\n",
    "import time\n",
    "import string\n",
    "import sys\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Dense, Dropout, Reshape, Flatten, concatenate, Input, Embedding, LeakyReLU\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f73f127",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/opc/my_functions.py:73: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data[cleaned_col] = data[cleaned_col].str.replace(pat, '')\n",
      "/home/opc/my_functions.py:74: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data[cleaned_col] = data[cleaned_col].str.replace(r'\\s+', ' ')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing completed in: 8 seconds.\n"
     ]
    }
   ],
   "source": [
    "df = process_dataset(load_thesis_datasets(), '2', '2-cleaned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3891f530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [attempt, make, devices, shown, video, informa...\n",
       "1     [oh, word, really, need, another, two, three, ...\n",
       "2     [hi, thanks, modern, gadgets, every, person, s...\n",
       "3     [hello, friends, video, show, heat, camping, t...\n",
       "4     [roundly, hated, widely, loved, ar15, rifle, s...\n",
       "                            ...                        \n",
       "92    [music, history, tells, us, adolf, hitler, dem...\n",
       "93    [folks, turbo, demand, truth, im, going, take,...\n",
       "94    [hey, everybody, welcome, back, listen, know, ...\n",
       "95    [hey, today, november, 23rd, 2020, im, federal...\n",
       "96    [ig12com, back, another, video, blog, one, wel...\n",
       "Name: tokens, Length: 578, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b813b6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customized version of our word2vec embedding creation function\n",
    "\n",
    "def get_word2vec(tokens_list, vector, generate_missing=False, k=300):\n",
    "    if len(tokens_list)<1:\n",
    "        return np.zeros(k)\n",
    "    if generate_missing:\n",
    "        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokens_list]\n",
    "    else:\n",
    "        vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokens_list]\n",
    "    length = len(vectorized)\n",
    "    summed = np.sum(vectorized, axis=0)\n",
    "    averaged = np.divide(summed, length)\n",
    "    return averaged\n",
    "\n",
    "def get_word2vec_embeddings100(vectors, clean_comments, generate_missing=False, k=100):\n",
    "    k = k\n",
    "    embeddings = clean_comments.apply(lambda x: get_word2vec(x, vectors, generate_missing=generate_missing, k=100))\n",
    "    return list(embeddings)\n",
    "\n",
    "def get_word2vec_embeddings200(vectors, clean_comments, generate_missing=False, k=200):\n",
    "    k = k\n",
    "    embeddings = clean_comments.apply(lambda x: get_word2vec(x, vectors, generate_missing=generate_missing, k=200))\n",
    "    return list(embeddings)\n",
    "\n",
    "def get_word2vec_embeddings300(vectors, clean_comments, generate_missing=False, k=300):\n",
    "    k = k\n",
    "    embeddings = clean_comments.apply(lambda x: get_word2vec(x, vectors, generate_missing=generate_missing, k=300))\n",
    "    return list(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "acbb4373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the word2vec models we have trained\n",
    "\n",
    "wikicorpus = 'glove.6B.200d.txt'\n",
    "\n",
    "\n",
    "reddit_sg_dim100_win10 = 'reddit_model_SG_dim100_con10.txt'\n",
    "reddit_sg_dim200_win3 = 'reddit_model_SG_dim200_win3.txt'\n",
    "reddit_sg_dim200_win5 = 'reddit_model_SG_dim200_win5.txt'\n",
    "reddit_sg_dim200_win10 = 'reddit_sg_dim300_win5.txt'\n",
    "reddit_sg_dim300_win5 = 'reddit_sg_dim300_win5.txt'\n",
    "reddit_sg_dim300_win10 = 'reddit_model_SG_dim300_win10.txt'\n",
    "reddit_cbow_dim100_win5 = 'reddit_model_CBOW_dim100_con5.txt'\n",
    "reddit_cbow_dim100_win10 = 'reddit_model_CBOW_dim200_con10.txt'\n",
    "reddit_cbow_dim200_win5 = 'reddit_CBOW_DIM200_win5.txt'\n",
    "reddit_cbow_dim200_win10 = 'reddit_model_CBOW_dim200_con10.txt'\n",
    "reddit_bow_dim300_win10 = 'reddit_model_CBOW_dim300_con10.txt'\n",
    "\n",
    "transcript_sg_dim100_win5 = 'transcript_SG_DIM100_win5.txt'\n",
    "transcript_sg_dim100_win10 = 'transcript_SG_DIM100_win10.txt'\n",
    "transcript_sg_dim200_win5 = 'transcript_SG_DIM200_win5.txt'\n",
    "transcript_sg_dim200_win10 = 'transcript_SG_DIM200_win10.txt'\n",
    "transcript_sg_dim300_win5 = 'transcript_SG_DIM300_WIN5.txt'\n",
    "transcript_sg_dim300_win10 = 'transcript_SG_DIM300_WIN10.txt'\n",
    "transcript_cbow_dim100_win5 = 'transcript_CBOW_DIM100_win5.txt'\n",
    "transcript_cbow_dim100_win10 = 'transcript_CBOW_DIM100_win10.txt'\n",
    "transcript_cbow_dim200_win5 = 'transcript_CBOW_DIM200_win5.txt'\n",
    "transcript_cbow_dim200_win10 = 'transcript_CBOW_DIM200_win10.txt'\n",
    "transcript_cbow_dim300_win5 = 'transcript_CBOW_DIM300_win5.txt'\n",
    "transcript_cbow_dim300_win10 = 'transcript_CBOW_DIM300_win10.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2a65332e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models and retrieve word vectors\n",
    "\n",
    "# REDDIT SKIP-GRAM MODELS\n",
    "\n",
    "# reddit_sg_dim100_win10_model = KeyedVectors.load(reddit_sg_dim100_win10)\n",
    "# reddit_sg_dim100_win10_vectors = reddit_sg_dim100_win10_model.wv\n",
    "\n",
    "# reddit_sg_dim200_win5_model = KeyedVectors.load(reddit_sg_dim200_win5)\n",
    "# reddit_sg_dim200_win5_vectors = reddit_sg_dim200_win5_model.wv\n",
    "\n",
    "# reddit_sg_dim200_win10_model = KeyedVectors.load(reddit_sg_dim200_win10)\n",
    "# reddit_sg_dim200_win10_vectors = reddit_sg_dim200_win10_model.wv\n",
    "\n",
    "# reddit_sg_dim300_win5_model = KeyedVectors.load(reddit_sg_dim300_win5)\n",
    "# reddit_sg_dim300_win5_vectors = reddit_sg_dim300_win5_model.wv\n",
    "\n",
    "# reddit_sg_dim300_win10_model = KeyedVectors.load(reddit_sg_dim300_win10) #NOT WORKING\n",
    "# reddit_sg_dim300_win10_vectors = reddit_sg_dim300_win10_model.wv\n",
    "\n",
    "# reddit_sg_dim100_win10_model = KeyedVectors.load(reddit_sg_dim300_win5)\n",
    "# reddit_sg_dim100_win10_vectors = reddit_sg_dim300_win5_model.wv\n",
    "\n",
    "# REDDIT CBOW MODELS\n",
    "\n",
    "# reddit_cbow_dim100_win5_model = KeyedVectors.load(reddit_cbow_dim100_win5)\n",
    "# reddit_cbow_dim100_win5_vectors = reddit_cbow_dim100_win5_model.wv\n",
    "\n",
    "# reddit_cbow_dim100_win10_model = KeyedVectors.load(reddit_cbow_dim100_win10)\n",
    "# reddit_cbow_dim100_win10_vectors = reddit_cbow_dim100_win10_model.wv\n",
    "\n",
    "# reddit_cbow_dim200_win5_model = KeyedVectors.load(reddit_cbow_dim200_win5)\n",
    "# reddit_cbow_dim200_win5_vectors = reddit_cbow_dim200_win5_model.wv\n",
    "\n",
    "# reddit_cbow_dim200_win10_model = KeyedVectors.load(reddit_cbow_dim200_win10)\n",
    "# reddit_cbow_dim200_win10_vectors = reddit_cbow_dim200_win10_model.wv\n",
    "\n",
    "# reddit_bow_dim300_win10_model = KeyedVectors.load(reddit_bow_dim300_win10)\n",
    "# reddit_bow_dim300_win10_vectors = reddit_bow_dim300_win10_model.wv\n",
    "\n",
    "# TRANSCRIPT SKIP-GRAM MODELS\n",
    "\n",
    "# transcript_sg_dim100_win5_model = KeyedVectors.load(transcript_sg_dim100_win5)\n",
    "# transcript_sg_dim100_win5_vectors = transcript_sg_dim100_win5_model.wv\n",
    "\n",
    "# transcript_sg_dim100_win10_model = KeyedVectors.load(transcript_sg_dim100_win10)\n",
    "# transcript_sg_dim100_win10_vectors = transcript_sg_dim100_win10_model.wv\n",
    "\n",
    "# transcript_sg_dim200_win5_model = KeyedVectors.load(transcript_sg_dim200_win5)\n",
    "# transcript_sg_dim200_win5_vectors = transcript_sg_dim200_win5_model.wv\n",
    "\n",
    "# transcript_sg_dim200_win10_model = KeyedVectors.load(transcript_sg_dim200_win10)\n",
    "# transcript_sg_dim200_win10_vectors = transcript_sg_dim200_win10_model.wv\n",
    "\n",
    "# transcript_sg_dim300_win5_model = KeyedVectors.load(transcript_sg_dim300_win5)\n",
    "# transcript_sg_dim300_win5_vectors = transcript_sg_dim300_win5_model.wv\n",
    "\n",
    "# transcript_sg_dim300_win10_model = KeyedVectors.load(transcript_sg_dim300_win10)\n",
    "# transcript_sg_dim300_win10_vectors = transcript_sg_dim300_win10_model.wv\n",
    "\n",
    "# TRANSCRIPT CBOW MODELS\n",
    "\n",
    "# transcript_cbow_dim100_win5_model = KeyedVectors.load(transcript_cbow_dim100_win5)\n",
    "# transcript_cbow_dim100_win5_vectors = transcript_cbow_dim100_win5_model.wv\n",
    "\n",
    "# transcript_cbow_dim100_win10_model = KeyedVectors.load(transcript_cbow_dim100_win10)\n",
    "# transcript_cbow_dim100_win10_vectors = transcript_cbow_dim100_win10_model.wv\n",
    "\n",
    "# transcript_cbow_dim200_win5_model = KeyedVectors.load(transcript_cbow_dim200_win5)\n",
    "# transcript_cbow_dim200_win5_vectors = transcript_cbow_dim200_win5_model.wv\n",
    "\n",
    "# transcript_cbow_dim200_win10_model = KeyedVectors.load(transcript_cbow_dim200_win10)\n",
    "# transcript_cbow_dim200_win10_vectors = transcript_cbow_dim200_win10_model.wv\n",
    "\n",
    "# transcript_cbow_dim300_win5_model = KeyedVectors.load(transcript_cbow_dim300_win5)\n",
    "# transcript_cbow_dim300_win5_vectors = transcript_cbow_dim300_win5_model.wv\n",
    "\n",
    "transcript_cbow_dim300_win10_model = KeyedVectors.load(transcript_cbow_dim300_win10)\n",
    "transcript_cbow_dim300_win10_vectors = transcript_cbow_dim300_win10_model.wv\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "208e4913",
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_X, Test_X, Train_Y, Test_Y = train_test_split(df['tokens'],df['3'],test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f1ac082",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/opc/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# Load WikiCorpus\n",
    "tmp_file = get_tmpfile(\"test_word2vec.txt\")\n",
    "_ = glove2word2vec(wikicorpus, tmp_file)\n",
    "wikicorpus_model = KeyedVectors.load_word2vec_format(tmp_file)\n",
    "\n",
    "# # # LOAD WIKICORPUS TRAINED WORD VECTORS\n",
    "\n",
    "# wikicorpus_training_embeddings = get_word2vec_embeddings200(wikicorpus_model, Train_X, generate_missing=True) \n",
    "# wikicorpus_test_embeddings = get_word2vec_embeddings200(wikicorpus_model, Test_X, generate_missing=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "494d98c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD WIKICORPUS TRAINED WORD VECTORS\n",
    "\n",
    "wikicorpus_training_embeddings = get_word2vec_embeddings200(wikicorpus_model, Train_X, generate_missing=True) \n",
    "wikicorpus_test_embeddings = get_word2vec_embeddings200(wikicorpus_model, Test_X, generate_missing=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a295937d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD REDDIT SKIP-GRAM TRAINED WORD VECTORS\n",
    "\n",
    "# reddit_sg_dim100_win10_training_embeddings = get_word2vec_embeddings100(transcript_sg_dim100_win10_vectors, Train_X, generate_missing=True) \n",
    "# reddit_sg_dim100_win10_test_embeddings = get_word2vec_embeddings100(reddit_sg_dim100_win10_vectors, Test_X, generate_missing=True)\n",
    "\n",
    "# reddit_sg_dim200_win5_training_embeddings = get_word2vec_embeddings200(reddit_sg_dim200_win5_vectors, Train_X, generate_missing=True, k=200) \n",
    "# reddit_sg_dim200_win5_test_embeddings = get_word2vec_embeddings200(reddit_sg_dim200_win5_vectors, Test_X, generate_missing=True, k=200)\n",
    "\n",
    "# reddit_sg_dim200_win10_training_embeddings = get_word2vec_embeddings200(reddit_sg_dim200_win10_vectors, Train_X, generate_missing=True, k=200) \n",
    "# reddit_sg_dim200_win10_test_embeddings = get_word2vec_embeddings200(reddit_sg_dim200_win10_vectors, Test_X, generate_missing=True, k=200)\n",
    "\n",
    "# reddit_sg_dim300_win5_training_embeddings = get_word2vec_embeddings300(reddit_sg_dim300_win5_vectors, Train_X, generate_missing=True, k=300) \n",
    "# reddit_sg_dim300_win5_test_embeddings = get_word2vec_embeddings300(reddit_sg_dim300_win5_vectors, Test_X, generate_missing=True, k=300)\n",
    "\n",
    "# reddit_sg_dim300_win10_training_embeddings = get_word2vec_embeddings300(reddit_sg_dim300_win10_vectors, Train_X, generate_missing=True, k=300) \n",
    "# reddit_sg_dim300_win10_test_embeddings = get_word2vec_embeddings300(reddit_sg_dim300_win10_vectors, Test_X, generate_missing=True, k=300)\n",
    "\n",
    "# # # LOAD REDDIT CBOW TRAINED WORD VECTORS\n",
    "\n",
    "# reddit_bow_dim100_win5_training_embeddings = get_word2vec_embeddings100(reddit_cbow_dim100_win5_vectors, Train_X, generate_missing=True, k=100) \n",
    "# reddit_bow_dim100_win5_test_embeddings = get_word2vec_embeddings100(reddit_cbow_dim100_win5_vectors, Test_X, generate_missing=True, k=100)\n",
    "\n",
    "# reddit_bow_dim100_win10_training_embeddings = get_word2vec_embeddings100(reddit_cbow_dim100_win10_vectors, Train_X, generate_missing=True, k=100) \n",
    "# reddit_bow_dim100_win10_test_embeddings = get_word2vec_embeddings100(reddit_cbow_dim100_win10_vectors, Test_X, generate_missing=True, k=100)\n",
    "\n",
    "# reddit_bow_dim200_win5_training_embeddings = get_word2vec_embeddings200(reddit_cbow_dim200_win5_vectors, Train_X, generate_missing=True, k=200) \n",
    "# reddit_bow_dim200_win5_test_embeddings = get_word2vec_embeddings200(reddit_cbow_dim200_win5_vectors, Test_X, generate_missing=True, k=200)\n",
    "\n",
    "# reddit_bow_dim200_win10_training_embeddings = get_word2vec_embeddings200(reddit_cbow_dim200_win10_vectors, Train_X, generate_missing=True, k=200) \n",
    "# reddit_bow_dim200_win10_test_embeddings = get_word2vec_embeddings200(reddit_cbow_dim200_win10_vectors, Test_X, generate_missing=True, k=200)\n",
    "\n",
    "# reddit_bow_dim300_win10_training_embeddings = get_word2vec_embeddings300(reddit_bow_dim300_win10_vectors, Train_X, generate_missing=True, k=300) \n",
    "# reddit_bow_dim300_win10_test_embeddings = get_word2vec_embeddings300(reddit_bow_dim300_win10_vectors, Test_X, generate_missing=True, k=300)\n",
    "\n",
    "# # # LOAD TRANSCRIPT SKIP-GRAM TRAINED WORD VECTORS\n",
    "\n",
    "# transcript_sg_dim100_win5_training_embeddings = get_word2vec_embeddings100(transcript_sg_dim100_win5_vectors, Train_X, generate_missing=True, k=100) \n",
    "# transcript_sg_dim100_win5_test_embeddings = get_word2vec_embeddings100(transcript_sg_dim100_win5_vectors, Test_X, generate_missing=True, k=100)\n",
    "\n",
    "# transcript_sg_dim100_win10_training_embeddings = get_word2vec_embeddings100(transcript_sg_dim100_win10_vectors, Train_X, generate_missing=True, k=100) \n",
    "# transcript_sg_dim100_win10_test_embeddings = get_word2vec_embeddings100(transcript_sg_dim100_win10_vectors, Test_X, generate_missing=True, k=100)\n",
    "\n",
    "# transcript_sg_dim200_win5_training_embeddings = get_word2vec_embeddings200(transcript_sg_dim200_win5_vectors, Train_X, generate_missing=True, k=200) \n",
    "# transcript_sg_dim200_win5_test_embeddings = get_word2vec_embeddings200(transcript_sg_dim200_win5_vectors, Test_X, generate_missing=True, k=200)\n",
    "\n",
    "# transcript_sg_dim200_win10_training_embeddings = get_word2vec_embeddings200(transcript_sg_dim200_win10_vectors, Train_X, generate_missing=True, k=200) \n",
    "# transcript_sg_dim200_win10_test_embeddings = get_word2vec_embeddings200(transcript_sg_dim200_win10_vectors, Test_X, generate_missing=True, k=200)\n",
    "\n",
    "# transcript_sg_dim300_win5_training_embeddings = get_word2vec_embeddings300(transcript_sg_dim300_win5_vectors, Train_X, generate_missing=True, k=300) \n",
    "# transcript_sg_dim300_win5_test_embeddings = get_word2vec_embeddings300(transcript_sg_dim300_win5_vectors, Test_X, generate_missing=True, k=300)\n",
    "\n",
    "# transcript_sg_dim300_win10_training_embeddings = get_word2vec_embeddings300(transcript_sg_dim300_win10_vectors, Train_X, generate_missing=True, k=300) \n",
    "# transcript_sg_dim300_win10_test_embeddings = get_word2vec_embeddings300(transcript_sg_dim300_win10_vectors, Test_X, generate_missing=True, k=300)\n",
    "\n",
    "# # # LOAD TRANSCRIPT CBOW TRAINED WORD VECTORS\n",
    "\n",
    "# transcript_cbow_dim100_win5_training_embeddings = get_word2vec_embeddings100(transcript_cbow_dim100_win5_vectors, Train_X, generate_missing=True, k=100) \n",
    "# transcript_cbow_dim100_win5_test_embeddings = get_word2vec_embeddings100(transcript_cbow_dim100_win5_vectors, Test_X, generate_missing=True, k=100)\n",
    "\n",
    "# transcript_cbow_dim100_win10_training_embeddings = get_word2vec_embeddings100(transcript_cbow_dim100_win10_vectors, Train_X, generate_missing=True, k=100) \n",
    "# transcript_cbow_dim100_win10_test_embeddings = get_word2vec_embeddings100(transcript_cbow_dim100_win10_vectors, Test_X, generate_missing=True, k=100)\n",
    "\n",
    "# transcript_cbow_dim200_win5_training_embeddings = get_word2vec_embeddings200(transcript_cbow_dim200_win5_vectors, Train_X, generate_missing=True, k=200) \n",
    "# transcript_cbow_dim200_win5_test_embeddings = get_word2vec_embeddings200(transcript_cbow_dim200_win5_vectors, Test_X, generate_missing=True, k=200)\n",
    "\n",
    "# transcript_cbow_dim200_win10_training_embeddings = get_word2vec_embeddings200(transcript_cbow_dim200_win10_vectors, Train_X, generate_missing=True, k=200) \n",
    "# transcript_cbow_dim200_win10_test_embeddings = get_word2vec_embeddings200(transcript_cbow_dim200_win10_vectors, Test_X, generate_missing=True, k=200)\n",
    "\n",
    "# transcript_cbow_dim300_win5_training_embeddings = get_word2vec_embeddings300(transcript_cbow_dim300_win5_vectors, Train_X, generate_missing=True, k=300) \n",
    "# transcript_cbow_dim300_win5_test_embeddings = get_word2vec_embeddings300(transcript_cbow_dim300_win5_vectors, Test_X, generate_missing=True, k=300)\n",
    "\n",
    "transcript_cbow_dim300_win10_training_embeddings = get_word2vec_embeddings300(transcript_cbow_dim300_win10_vectors, Train_X, generate_missing=True, k=300) \n",
    "transcript_cbow_dim300_win10_test_embeddings = get_word2vec_embeddings300(transcript_cbow_dim300_win10_vectors, Test_X, generate_missing=True, k=300)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a0b45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load WikiCorpus\n",
    "tmp_file = get_tmpfile(\"test_word2vec.txt\")\n",
    "_ = glove2word2vec(wikicorpus, tmp_file)\n",
    "wikicorpus_model = KeyedVectors.load_word2vec_format(tmp_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bbd60945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding\n",
    "Encoder = LabelEncoder()\n",
    "Train_Y = Encoder.fit_transform(Train_Y)\n",
    "Test_Y = Encoder.fit_transform(Test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e57918",
   "metadata": {},
   "source": [
    "# REDDIT SKIP-GRAM WORD VECTORS TESTED AGAINST SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d2adcf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter setting\n",
    "C = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2a7664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Preparing our SVM Classifier for tuning\n",
    "# # fit the training dataset on the classifier\n",
    "# SVM = svm.SVC(C=1.1, kernel='linear', degree=10, gamma='auto')\n",
    "# SVM.fit(reddit_sg_dim100_win10_training_embeddings,Train_Y)\n",
    "# # predict the labels on validation dataset\n",
    "# predictions_SVM = SVM.predict(reddit_sg_dim100_win10_test_embeddings)\n",
    "# # Use accuracy_score function to get the accuracy\n",
    "# print(\"Model:\", 'reddit_sg_dim100_win10', 'results...')\n",
    "# print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, Test_Y)*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2d179f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing our SVM Classifier for tuning\n",
    "# fit the training dataset on the classifier\n",
    "SVM = svm.SVC(C=C, kernel='linear', degree=10, gamma='auto')\n",
    "SVM.fit(reddit_sg_dim200_win5_training_embeddings,Train_Y)\n",
    "# predict the labels on validation dataset\n",
    "predictions_SVM = SVM.predict(reddit_sg_dim200_win5_test_embeddings)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"Model:\", 'reddit_sg_dim200_win5', 'results...')\n",
    "print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, Test_Y)*100)\n",
    "print(\"F1-Score ->\", f1_score(predictions_SVM, Test_Y, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba560b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Preparing our SVM Classifier for tuning\n",
    "# # fit the training dataset on the classifier\n",
    "# SVM = svm.SVC(C=1, kernel='linear', degree=10, gamma='auto')\n",
    "# SVM.fit(reddit_sg_dim200_win10_training_embeddings,Train_Y)\n",
    "# # predict the labels on validation dataset\n",
    "# predictions_SVM = SVM.predict(reddit_sg_dim200_win10_test_embeddings)\n",
    "# # Use accuracy_score function to get the accuracy\n",
    "# print(\"Model:\", 'reddit_sg_dim200_win10', 'results...')\n",
    "# print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, Test_Y)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c023c8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Preparing our SVM Classifier for tuning\n",
    "# fit the training dataset on the classifier\n",
    "SVM = svm.SVC(C=C, kernel='linear', degree=10, gamma='auto')\n",
    "SVM.fit(reddit_sg_dim300_win5_training_embeddings,Train_Y)\n",
    "# predict the labels on validation dataset\n",
    "predictions_SVM = SVM.predict(reddit_sg_dim300_win5_test_embeddings)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"Model:\", 'reddit_sg_dim300_win5', 'results...')\n",
    "print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, Test_Y)*100)\n",
    "print(\"F1-Score ->\", f1_score(predictions_SVM, Test_Y, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c27110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing our SVM Classifier for tuning\n",
    "# fit the training dataset on the classifier\n",
    "SVM = svm.SVC(C=C, kernel='linear', degree=10, gamma='auto')\n",
    "SVM.fit(reddit_sg_dim300_win10_training_embeddings,Train_Y)\n",
    "# predict the labels on validation dataset\n",
    "predictions_SVM = SVM.predict(reddit_sg_dim300_win10_test_embeddings)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"Model:\", 'reddit_sg_dim300_win10', 'results...')\n",
    "print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, Test_Y)*100)\n",
    "print(\"F1-Score ->\", f1_score(predictions_SVM, Test_Y, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a212433",
   "metadata": {},
   "source": [
    "# REDDIT CBOW WORD VECTORS TESTED AGAINST SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6277de72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing our SVM Classifier for tuning\n",
    "# fit the training dataset on the classifier\n",
    "SVM = svm.SVC(C=C, kernel='linear', degree=10, gamma='auto')\n",
    "SVM.fit(reddit_bow_dim100_win5_training_embeddings,Train_Y)\n",
    "# predict the labels on validation dataset\n",
    "predictions_SVM = SVM.predict(reddit_bow_dim100_win5_test_embeddings)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"Model:\", 'reddit_bow_dim100_win5', 'results...')\n",
    "print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, Test_Y)*100)\n",
    "print(\"F1-Score ->\", f1_score(predictions_SVM, Test_Y, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab78e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Preparing our SVM Classifier for tuning\n",
    "# # fit the training dataset on the classifier\n",
    "# SVM = svm.SVC(C=C, kernel='linear', degree=10, gamma='auto')\n",
    "# SVM.fit(reddit_bow_dim100_win10_training_embeddings,Train_Y)\n",
    "# # predict the labels on validation dataset\n",
    "# predictions_SVM = SVM.predict(reddit_bow_dim100_win10_test_embeddings)\n",
    "# # Use accuracy_score function to get the accuracy\n",
    "# print(\"Model:\", 'reddit_bow_dim100_win10', 'results...')\n",
    "# print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, Test_Y)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03a6977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Preparing our SVM Classifier for tuning\n",
    "# # fit the training dataset on the classifier\n",
    "# SVM = svm.SVC(C=1, kernel='linear', degree=10, gamma='auto')\n",
    "# SVM.fit(reddit_bow_dim200_win5_training_embeddings,Train_Y)\n",
    "# # predict the labels on validation dataset\n",
    "# predictions_SVM = SVM.predict(reddit_bow_dim200_win5_test_embeddings)\n",
    "# # Use accuracy_score function to get the accuracy\n",
    "# print(\"Model:\", 'reddit_bow_dim200_win5', 'results...')\n",
    "# print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, Test_Y)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089cd5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing our SVM Classifier for tuning\n",
    "# fit the training dataset on the classifier\n",
    "SVM = svm.SVC(C=C, kernel='linear', degree=10, gamma='auto')\n",
    "SVM.fit(reddit_bow_dim200_win10_training_embeddings,Train_Y)\n",
    "# predict the labels on validation dataset\n",
    "predictions_SVM = SVM.predict(reddit_bow_dim200_win10_test_embeddings)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"Model:\", 'reddit_bow_dim200_win10', 'results...')\n",
    "print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, Test_Y)*100)\n",
    "print(\"F1-Score ->\", f1_score(predictions_SVM, Test_Y, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885a230a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing our SVM Classifier for tuning\n",
    "# fit the training dataset on the classifier\n",
    "SVM = svm.SVC(C=C, kernel='linear', degree=10, gamma='auto')\n",
    "SVM.fit(reddit_bow_dim300_win10_training_embeddings,Train_Y)\n",
    "# predict the labels on validation dataset\n",
    "predictions_SVM = SVM.predict(reddit_bow_dim300_win10_test_embeddings)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"Model:\", 'reddit_bow_dim300_win10', 'results...')\n",
    "print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, Test_Y)*100)\n",
    "print(\"F1-Score ->\", f1_score(predictions_SVM, Test_Y, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b5481e",
   "metadata": {},
   "source": [
    "# TRANSCRIPT SKIP-GRAM WORD VECTORS TESTED AGAINST SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a6d4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing our SVM Classifier for tuning\n",
    "# fit the training dataset on the classifier\n",
    "SVM = svm.SVC(C=C, kernel='linear', degree=10, gamma='auto')\n",
    "SVM.fit(transcript_sg_dim100_win5_training_embeddings,Train_Y)\n",
    "# predict the labels on validation dataset\n",
    "predictions_SVM = SVM.predict(transcript_sg_dim100_win5_test_embeddings)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"Model:\", 'transcript_sg_dim100_win5', 'results...')\n",
    "print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, Test_Y)*100)\n",
    "print(\"F1-Score ->\", f1_score(predictions_SVM, Test_Y, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dd28ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing our SVM Classifier for tuning\n",
    "# fit the training dataset on the classifier\n",
    "SVM = svm.SVC(C=C, kernel='linear', degree=10, gamma='auto')\n",
    "SVM.fit(transcript_sg_dim100_win10_training_embeddings,Train_Y)\n",
    "# predict the labels on validation dataset\n",
    "predictions_SVM = SVM.predict(transcript_sg_dim100_win10_test_embeddings)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"Model:\", 'transcript_sg_dim100_win10', 'results...')\n",
    "print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, Test_Y)*100)\n",
    "print(\"F1-Score ->\", f1_score(predictions_SVM, Test_Y, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f658079e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Preparing our SVM Classifier for tuning\n",
    "# # fit the training dataset on the classifier\n",
    "# SVM = svm.SVC(C=1, kernel='linear', degree=10, gamma='auto')\n",
    "# SVM.fit(transcript_sg_dim200_win5_training_embeddings,Train_Y)\n",
    "# # predict the labels on validation dataset\n",
    "# predictions_SVM = SVM.predict(transcript_sg_dim200_win5_test_embeddings)\n",
    "# # Use accuracy_score function to get the accuracy\n",
    "# print(\"Model:\", 'transcript_sg_dim200_win5', 'results...')\n",
    "# print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, Test_Y)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2a6310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Preparing our SVM Classifier for tuning\n",
    "# # fit the training dataset on the classifier\n",
    "# SVM = svm.SVC(C=1, kernel='linear', degree=10, gamma='auto')\n",
    "# SVM.fit(transcript_sg_dim200_win10_training_embeddings,Train_Y)\n",
    "# # predict the labels on validation dataset\n",
    "# predictions_SVM = SVM.predict(transcript_sg_dim200_win10_test_embeddings)\n",
    "# # Use accuracy_score function to get the accuracy\n",
    "# print(\"Model:\", 'transcript_sg_dim200_win10', 'results...')\n",
    "# print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, Test_Y)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97777a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing our SVM Classifier for tuning\n",
    "# fit the training dataset on the classifier\n",
    "SVM = svm.SVC(C=C, kernel='linear', degree=10, gamma='auto')\n",
    "SVM.fit(transcript_sg_dim300_win5_training_embeddings,Train_Y)\n",
    "# predict the labels on validation dataset\n",
    "predictions_SVM = SVM.predict(transcript_sg_dim300_win5_test_embeddings)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"Model:\", 'transcript_sg_dim300_win5', 'results...')\n",
    "print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, Test_Y)*100)\n",
    "print(\"F1-Score ->\", f1_score(predictions_SVM, Test_Y, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce42e74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing our SVM Classifier for tuning\n",
    "# fit the training dataset on the classifier\n",
    "SVM = svm.SVC(C=C, kernel='linear', degree=10, gamma='auto')\n",
    "SVM.fit(transcript_sg_dim300_win10_training_embeddings,Train_Y)\n",
    "# predict the labels on validation dataset\n",
    "predictions_SVM = SVM.predict(transcript_sg_dim300_win10_test_embeddings)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"Model:\", 'transcript_sg_dim300_win10', 'results...')\n",
    "print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, Test_Y)*100)\n",
    "print(\"F1-Score ->\", f1_score(predictions_SVM, Test_Y, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a331cf",
   "metadata": {},
   "source": [
    "# TRASCRIPT CBOW WORD VECTORS TESTED AGAINST SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6547757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing our SVM Classifier for tuning\n",
    "# fit the training dataset on the classifier\n",
    "SVM = svm.SVC(C=C, kernel='linear', degree=10, gamma='auto')\n",
    "SVM.fit(transcript_cbow_dim100_win5_training_embeddings,Train_Y)\n",
    "# predict the labels on validation dataset\n",
    "predictions_SVM = SVM.predict(transcript_cbow_dim100_win5_test_embeddings)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"Model:\", 'transcript_cbow_dim100_win5', 'results...')\n",
    "print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, Test_Y)*100)\n",
    "print(\"F1-Score ->\", f1_score(predictions_SVM, Test_Y, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed72fca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing our SVM Classifier for tuning\n",
    "# fit the training dataset on the classifier\n",
    "SVM = svm.SVC(C=C, kernel='linear', degree=10, gamma='auto')\n",
    "SVM.fit(transcript_cbow_dim100_win10_training_embeddings,Train_Y)\n",
    "# predict the labels on validation dataset\n",
    "predictions_SVM = SVM.predict(transcript_cbow_dim100_win10_test_embeddings)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"Model:\", 'transcript_cbow_dim100_win10', 'results...')\n",
    "print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, Test_Y)*100)\n",
    "print(\"F1-Score ->\", f1_score(predictions_SVM, Test_Y, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b959691a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing our SVM Classifier for tuning\n",
    "# fit the training dataset on the classifier\n",
    "SVM = svm.SVC(C=C, kernel='linear', degree=10, gamma='auto')\n",
    "SVM.fit(transcript_cbow_dim200_win5_training_embeddings,Train_Y)\n",
    "# predict the labels on validation dataset\n",
    "predictions_SVM = SVM.predict(transcript_cbow_dim200_win5_test_embeddings)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"Model:\", 'transcript_cbow_dim200_win5', 'results...')\n",
    "print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, Test_Y)*100)\n",
    "print(\"F1-Score ->\", f1_score(predictions_SVM, Test_Y, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb65a556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing our SVM Classifier for tuning\n",
    "# fit the training dataset on the classifier\n",
    "SVM = svm.SVC(C=C, kernel='linear', degree=10, gamma='auto')\n",
    "SVM.fit(transcript_cbow_dim200_win10_training_embeddings,Train_Y)\n",
    "# predict the labels on validation dataset\n",
    "predictions_SVM = SVM.predict(transcript_cbow_dim200_win10_test_embeddings)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"Model:\", 'transcript_cbow_dim200_win10', 'results...')\n",
    "print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, Test_Y)*100)\n",
    "print(\"F1-Score ->\", f1_score(predictions_SVM, Test_Y, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abd5dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing our SVM Classifier for tuning\n",
    "# fit the training dataset on the classifier\n",
    "SVM = svm.SVC(C=C, kernel='linear', degree=10, gamma='auto')\n",
    "SVM.fit(transcript_cbow_dim300_win5_training_embeddings,Train_Y)\n",
    "# predict the labels on validation dataset\n",
    "predictions_SVM = SVM.predict(transcript_cbow_dim300_win5_test_embeddings)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"Model:\", 'transcript_cbow_dim300_win5', 'results...')\n",
    "print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, Test_Y)*100)\n",
    "print(\"F1-Score ->\", f1_score(predictions_SVM, Test_Y, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "10071407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: transcript_cbow_dim300_win10 results...\n",
      "SVM Accuracy Score ->  87.93103448275862\n"
     ]
    }
   ],
   "source": [
    "# Preparing our SVM Classifier for tuning\n",
    "# fit the training dataset on the classifier\n",
    "SVM = svm.SVC(C=1, kernel='linear', degree=10, gamma='auto')\n",
    "SVM.fit(transcript_cbow_dim300_win10_training_embeddings,Train_Y)\n",
    "# predict the labels on validation dataset\n",
    "predictions_SVM = SVM.predict(transcript_cbow_dim300_win10_test_embeddings)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"Model:\", 'transcript_cbow_dim300_win10', 'results...')\n",
    "print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, Test_Y)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9c2f87",
   "metadata": {},
   "source": [
    "# WIKICORPUS WORD VECTORS TESTED AGAINST SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "797415ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 1.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4df393e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: WikiCorpus results...\n",
      "SVM Accuracy Score ->  81.89655172413794\n",
      "F1-Score -> 0.8139747995418098\n"
     ]
    }
   ],
   "source": [
    "# Preparing our SVM Classifier for tuning\n",
    "# fit the training dataset on the classifier\n",
    "SVM = svm.SVC(C=C, kernel='linear', degree=10, gamma='auto')\n",
    "SVM.fit(wikicorpus_training_embeddings,Train_Y)\n",
    "# predict the labels on validation dataset\n",
    "predictions_SVM = SVM.predict(wikicorpus_test_embeddings)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"Model:\", 'WikiCorpus', 'results...')\n",
    "print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, Test_Y)*100)\n",
    "print(\"F1-Score ->\", f1_score(predictions_SVM, Test_Y, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34252477",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
